{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMN0aJo6TClsIPs5hybMgrE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahilsait/credit-risk-assessment-using-GNNs/blob/main/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeiNyCQi3Oj4",
        "outputId": "9f7ed84c-8202-41b5-fb5b-f070fbc62f46"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "un45CCmU2VAB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GraphSAGE, TopKPooling\n",
        "from torch_geometric.nn import global_mean_pool"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CreditRiskGNN(torch.nn.Module):\n",
        "    def __init__(self, num_features=1, hidden_dim=32, num_classes=3):\n",
        "        \"\"\"\n",
        "        Initialize the GNN model\n",
        "\n",
        "        Args:\n",
        "            num_features: Number of features per node (1 as per your graph)\n",
        "            hidden_dim: Size of hidden layers (32 as per paper)\n",
        "            num_classes: Number of risk categories (3: low, medium, high)\n",
        "        \"\"\"\n",
        "        super(CreditRiskGNN, self).__init__()\n",
        "\n",
        "        # Step 1: Three GraphSAGE layers\n",
        "        self.sage1 = GraphSAGE(\n",
        "            in_channels=num_features,  # Input features per node\n",
        "            hidden_channels=hidden_dim, # Hidden dimension size\n",
        "            num_layers=1               # Single GraphSAGE layer\n",
        "        )\n",
        "\n",
        "        self.sage2 = GraphSAGE(\n",
        "            in_channels=hidden_dim,\n",
        "            hidden_channels=hidden_dim,\n",
        "            num_layers=1\n",
        "        )\n",
        "\n",
        "        self.sage3 = GraphSAGE(\n",
        "            in_channels=hidden_dim,\n",
        "            hidden_channels=hidden_dim,\n",
        "            num_layers=1\n",
        "        )\n",
        "\n",
        "        # Step 2: TopK pooling layer\n",
        "        self.pool = TopKPooling(hidden_dim)\n",
        "\n",
        "        # Step 3: Final prediction layers (MLP)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            x: Node features [num_nodes, num_features]\n",
        "            edge_index: Graph connectivity [2, num_edges]\n",
        "            batch: Batch assignments for nodes [num_nodes]\n",
        "        \"\"\"\n",
        "        # Step 1: Apply GraphSAGE layers with ReLU\n",
        "        x = F.relu(self.sage1(x, edge_index))\n",
        "        x = F.relu(self.sage2(x, edge_index))\n",
        "        x = F.relu(self.sage3(x, edge_index))\n",
        "\n",
        "        # Step 2: Apply pooling\n",
        "        x, edge_index, _, batch, _, _ = self.pool(x, edge_index, None, batch)\n",
        "\n",
        "        # Step 3: Global mean pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Step 4: MLP classifier\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Step 5: Log softmax for classification\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Function to verify model\n",
        "def verify_model():\n",
        "    \"\"\"\n",
        "    Create and verify the model with sample data\n",
        "    \"\"\"\n",
        "    # Create model\n",
        "    model = CreditRiskGNN(num_features=1)\n",
        "    print(\"Model Architecture:\")\n",
        "    print(model)\n",
        "\n",
        "    # Create sample data\n",
        "    num_nodes = 20  # 20 financial indicators\n",
        "    x = torch.randn(num_nodes, 1)  # Random features\n",
        "    edge_index = torch.randint(0, num_nodes, (2, 38))  # Random edges\n",
        "    batch = torch.zeros(num_nodes, dtype=torch.long)  # All nodes in same batch\n",
        "\n",
        "    # Test forward pass\n",
        "    try:\n",
        "        out = model(x, edge_index, batch)\n",
        "        print(\"\\nForward pass successful!\")\n",
        "        print(\"Input shape:\", x.shape)\n",
        "        print(\"Output shape:\", out.shape)\n",
        "        print(\"Output values (log probabilities):\", out)\n",
        "    except Exception as e:\n",
        "        print(\"Error in forward pass:\", str(e))\n",
        "\n",
        "# Create and test model\n",
        "def main():\n",
        "    print(\"Testing model architecture...\")\n",
        "\n",
        "    try:\n",
        "        verify_model()\n",
        "\n",
        "        # Additional information\n",
        "        print(\"\\nModel Information:\")\n",
        "        print(\"- Input: Financial indicators as nodes\")\n",
        "        print(\"- Hidden layers: 3 GraphSAGE layers\")\n",
        "        print(\"- Pooling: TopK pooling\")\n",
        "        print(\"- Output: 3 risk categories (low, medium, high)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", str(e))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "aSkkdP6v2omt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1d7c19-4bad-4a2b-f03c-50a64aaac2b5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing model architecture...\n",
            "Model Architecture:\n",
            "CreditRiskGNN(\n",
            "  (sage1): GraphSAGE(1, 32, num_layers=1)\n",
            "  (sage2): GraphSAGE(32, 32, num_layers=1)\n",
            "  (sage3): GraphSAGE(32, 32, num_layers=1)\n",
            "  (pool): TopKPooling(32, ratio=0.5, multiplier=1.0)\n",
            "  (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=3, bias=True)\n",
            ")\n",
            "\n",
            "Forward pass successful!\n",
            "Input shape: torch.Size([20, 1])\n",
            "Output shape: torch.Size([1, 3])\n",
            "Output values (log probabilities): tensor([[-1.1496, -1.0495, -1.0992]], grad_fn=<LogSoftmaxBackward0>)\n",
            "\n",
            "Model Information:\n",
            "- Input: Financial indicators as nodes\n",
            "- Hidden layers: 3 GraphSAGE layers\n",
            "- Pooling: TopK pooling\n",
            "- Output: 3 risk categories (low, medium, high)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhnhQIiP4DM2",
        "outputId": "133b4cc2-82c7-4146-c438-cec2b60f92f7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_zscore(df):\n",
        "    \"\"\"\n",
        "    Calculate Altman Z-score\n",
        "    \"\"\"\n",
        "    # Z-score components\n",
        "    df['z1'] = (df['act'] - df['lct']) / df['at']  # Working Capital/Total Assets\n",
        "    df['z2'] = df['ni'] / df['at']                 # Retained Earnings/Total Assets\n",
        "    df['z3'] = df['oibdp'] / df['at']             # EBIT/Total Assets\n",
        "    df['z4'] = df['ceq'] / df['lt']               # Equity/Total Liabilities\n",
        "    df['z5'] = df['sale'] / df['at']              # Sales/Total Assets\n",
        "\n",
        "    # Altman Z-score formula\n",
        "    z_score = (\n",
        "        1.2 * df['z1'] +\n",
        "        1.4 * df['z2'] +\n",
        "        3.3 * df['z3'] +\n",
        "        0.6 * df['z4'] +\n",
        "        1.0 * df['z5']\n",
        "    )\n",
        "\n",
        "    return z_score"
      ],
      "metadata": {
        "id": "ig31ZYWQFqnn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_risk_category(z_score):\n",
        "    \"\"\"\n",
        "    Convert Z-score to risk category\n",
        "    \"\"\"\n",
        "    if z_score < 1.81:\n",
        "        return 2  # High risk\n",
        "    elif z_score < 2.99:\n",
        "        return 1  # Medium risk\n",
        "    else:\n",
        "        return 0  # Low risk"
      ],
      "metadata": {
        "id": "z3edWjUEFtXb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_with_labels(graph_data_list, df):\n",
        "    \"\"\"\n",
        "    Add risk category labels to graphs\n",
        "    \"\"\"\n",
        "    # Calculate Z-scores\n",
        "    df['z_score'] = calculate_zscore(df)\n",
        "\n",
        "    # Convert to risk categories\n",
        "    df['risk_category'] = df['z_score'].apply(get_risk_category)\n",
        "\n",
        "    # Sort DataFrame to match graph order\n",
        "    df_sorted = df.sort_values(['gvkey', 'fyear'])\n",
        "\n",
        "    # Add labels to graphs\n",
        "    for i, graph in enumerate(graph_data_list):\n",
        "        graph.y = torch.tensor([df_sorted['risk_category'].iloc[i]], dtype=torch.long)\n",
        "\n",
        "    # Print distribution\n",
        "    print(\"\\nRisk Category Distribution:\")\n",
        "    print(df_sorted['risk_category'].value_counts().sort_index())\n",
        "\n",
        "    return graph_data_list"
      ],
      "metadata": {
        "id": "tf8Jr3t-FzL6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/datasets/preprocessed_data.csv')  # Load your original data\n",
        "\n",
        "# 3. Add labels to graphs\n",
        "graph_data_list = torch.load('/content/drive/MyDrive/datasets/graphs.pt')\n",
        "graph_data_list = prepare_data_with_labels(graph_data_list, df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n6vH1BUF2rN",
        "outputId": "92ced0bb-4d04-4d43-adcb-4b218f88b0ba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-edefb6767e2a>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  graph_data_list = torch.load('/content/drive/MyDrive/datasets/graphs.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Risk Category Distribution:\n",
            "risk_category\n",
            "0    44246\n",
            "1      824\n",
            "2     4822\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def train_model():\n",
        "    # 2. Split into train/val/test\n",
        "    train_data, test_data = train_test_split(\n",
        "        graph_data_list,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_data, val_data = train_test_split(\n",
        "        train_data,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"\\nSplit sizes:\")\n",
        "    print(f\"Train: {len(train_data)}\")\n",
        "    print(f\"Validation: {len(val_data)}\")\n",
        "    print(f\"Test: {len(test_data)}\")\n",
        "\n",
        "\n",
        "    # 3. Create data loaders\n",
        "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=32)\n",
        "    test_loader = DataLoader(test_data, batch_size=32)\n",
        "\n",
        "    # 4. Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    model = CreditRiskGNN(\n",
        "        num_features=1,      # Each node has 1 feature\n",
        "        hidden_dim=32,       # Hidden dimension size\n",
        "        num_classes=3        # Risk categories\n",
        "    ).to(device)\n",
        "\n",
        "    # 5. Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # 6. Training loop\n",
        "    best_val_acc = 0\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "    num_epochs = 100\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = F.nll_loss(out, batch.y)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                pred = out.argmax(dim=1)\n",
        "                correct += pred.eq(batch.y).sum().item()\n",
        "                total += batch.y.size(0)\n",
        "\n",
        "        val_acc = correct / total\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch: {epoch+1:03d}, Loss: {avg_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    # 6. Test phase\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "    model.eval()\n",
        "\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            pred = out.argmax(dim=1)\n",
        "            test_correct += pred.eq(batch.y).sum().item()\n",
        "            test_total += batch.y.size(0)\n",
        "\n",
        "            predictions.extend(pred.cpu().numpy())\n",
        "            true_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    test_acc = test_correct / test_total\n",
        "\n",
        "    # 7. Print results\n",
        "    print(\"\\nFinal Results:\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predictions,\n",
        "                              target_names=['Low Risk', 'Medium Risk', 'High Risk']))\n",
        "\n",
        "    return model, test_acc\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        print(\"Starting model training...\")\n",
        "        model, test_acc = train_model()\n",
        "        print(\"\\nTraining completed successfully!\")\n",
        "        print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clNkpgwx3eeW",
        "outputId": "4197789c-564e-4003-bece-8a90bfc93317"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "\n",
            "Split sizes:\n",
            "Train: 31930\n",
            "Validation: 7983\n",
            "Test: 9979\n",
            "\n",
            "Using device: cpu\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100: 100%|██████████| 998/998 [00:19<00:00, 50.58it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 0.2685, Val Acc: 0.9088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/100: 100%|██████████| 998/998 [00:09<00:00, 105.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 002, Loss: 0.2001, Val Acc: 0.9168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/100: 100%|██████████| 998/998 [00:09<00:00, 107.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 003, Loss: 0.1891, Val Acc: 0.9220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/100: 100%|██████████| 998/998 [00:10<00:00, 96.63it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 004, Loss: 0.1812, Val Acc: 0.9248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/100: 100%|██████████| 998/998 [00:10<00:00, 95.74it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 005, Loss: 0.1751, Val Acc: 0.9226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/100: 100%|██████████| 998/998 [00:10<00:00, 99.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 006, Loss: 0.1712, Val Acc: 0.9277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/100: 100%|██████████| 998/998 [00:08<00:00, 113.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 007, Loss: 0.1671, Val Acc: 0.9314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/100: 100%|██████████| 998/998 [00:09<00:00, 101.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 008, Loss: 0.1655, Val Acc: 0.9306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/100: 100%|██████████| 998/998 [00:10<00:00, 96.37it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 009, Loss: 0.1720, Val Acc: 0.9340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/100: 100%|██████████| 998/998 [00:10<00:00, 97.62it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 010, Loss: 0.1654, Val Acc: 0.9359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/100: 100%|██████████| 998/998 [00:09<00:00, 104.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 011, Loss: 0.1611, Val Acc: 0.9291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/100: 100%|██████████| 998/998 [00:08<00:00, 112.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 012, Loss: 0.1624, Val Acc: 0.9330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/100: 100%|██████████| 998/998 [00:10<00:00, 97.54it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 013, Loss: 0.1584, Val Acc: 0.9345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/100: 100%|██████████| 998/998 [00:10<00:00, 99.38it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 014, Loss: 0.1585, Val Acc: 0.9336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/100: 100%|██████████| 998/998 [00:09<00:00, 102.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 015, Loss: 0.1575, Val Acc: 0.9315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/100: 100%|██████████| 998/998 [00:10<00:00, 96.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 016, Loss: 0.1560, Val Acc: 0.9327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/100: 100%|██████████| 998/998 [00:09<00:00, 105.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 017, Loss: 0.1570, Val Acc: 0.9292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/100: 100%|██████████| 998/998 [00:10<00:00, 97.39it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 018, Loss: 0.1558, Val Acc: 0.9326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/100: 100%|██████████| 998/998 [00:10<00:00, 98.29it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 019, Loss: 0.1575, Val Acc: 0.9355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/100: 100%|██████████| 998/998 [00:09<00:00, 104.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 020, Loss: 0.1531, Val Acc: 0.9364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/100: 100%|██████████| 998/998 [00:08<00:00, 113.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 021, Loss: 0.1526, Val Acc: 0.9221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/100: 100%|██████████| 998/998 [00:10<00:00, 97.90it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 022, Loss: 0.1524, Val Acc: 0.9372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/100: 100%|██████████| 998/998 [00:10<00:00, 98.76it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 023, Loss: 0.1546, Val Acc: 0.9331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/100: 100%|██████████| 998/998 [00:10<00:00, 98.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 024, Loss: 0.1588, Val Acc: 0.9346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/100: 100%|██████████| 998/998 [00:09<00:00, 106.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 025, Loss: 0.1493, Val Acc: 0.9329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/100: 100%|██████████| 998/998 [00:09<00:00, 103.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 026, Loss: 0.1490, Val Acc: 0.9326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/100: 100%|██████████| 998/998 [00:10<00:00, 96.18it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 027, Loss: 0.1488, Val Acc: 0.9337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/100: 100%|██████████| 998/998 [00:10<00:00, 97.16it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 028, Loss: 0.1505, Val Acc: 0.9321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/100: 100%|██████████| 998/998 [00:09<00:00, 108.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 029, Loss: 0.1520, Val Acc: 0.9364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/100: 100%|██████████| 998/998 [00:09<00:00, 110.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 030, Loss: 0.1509, Val Acc: 0.9317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/100: 100%|██████████| 998/998 [00:10<00:00, 97.11it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 031, Loss: 0.1524, Val Acc: 0.9354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/100: 100%|██████████| 998/998 [00:10<00:00, 97.92it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 032, Loss: 0.1499, Val Acc: 0.9342\n",
            "Early stopping!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-a9f4a08093d7>:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results:\n",
            "Best Validation Accuracy: 0.9372\n",
            "Test Accuracy: 0.9382\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Low Risk       0.97      0.98      0.97      8854\n",
            " Medium Risk       0.00      0.00      0.00       164\n",
            "   High Risk       0.71      0.76      0.73       961\n",
            "\n",
            "    accuracy                           0.94      9979\n",
            "   macro avg       0.56      0.58      0.57      9979\n",
            "weighted avg       0.92      0.94      0.93      9979\n",
            "\n",
            "\n",
            "Training completed successfully!\n",
            "Final Test Accuracy: 0.9382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}